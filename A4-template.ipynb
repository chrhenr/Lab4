{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f0fea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56578f6f",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1761f4d2",
   "metadata": {},
   "source": [
    "Loading the synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aacba01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to edit the path, depending on where you put the files.\n",
    "data = pd.read_csv('./data/a4_synthetic.csv')\n",
    "\n",
    "X = data.drop(columns='y').to_numpy()\n",
    "Y = data.y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b166d6ba",
   "metadata": {},
   "source": [
    "Training a linear regression model for this synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "111ff34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE = 0.7999661130823178\n",
      "Epoch 2: MSE = 0.017392390107906875\n",
      "Epoch 3: MSE = 0.009377418010839892\n",
      "Epoch 4: MSE = 0.009355326971438456\n",
      "Epoch 5: MSE = 0.009365440968904256\n",
      "Epoch 6: MSE = 0.009366989180952533\n",
      "Epoch 7: MSE = 0.009367207398577986\n",
      "Epoch 8: MSE = 0.009367238983974489\n",
      "Epoch 9: MSE = 0.009367243704122532\n",
      "Epoch 10: MSE = 0.009367244427185763\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "w_init = np.random.normal(size=(2, 1)) #loc=0.0:Mean (“centre”) of the distribution, scale=1.0:Standard deviation, size=(2, 1) ->matrix= [[0,0]] 2x1\n",
    "b_init = np.random.normal(size=(1, 1))\n",
    "\n",
    "# We just declare the parameter tensors. Do not use nn.Linear.\n",
    "w = torch.tensor(w_init, requires_grad = True)# a tensor initialized as w_init This is the weight\n",
    "b = torch.tensor(b_init, requires_grad = True)# a tensor initialized as b_init This is the bias\n",
    "\n",
    "eta = 1e-2\n",
    "opt = torch.optim.SGD(params=[w,b] ,lr=eta)# a SGD optimizer with a learning rate of eta\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    sum_err = 0\n",
    "\n",
    "    for row in range(X.shape[0]):\n",
    "        opt.zero_grad()\n",
    "        x = torch.tensor(X[[row], :])\n",
    "                                     \n",
    "        y = torch.tensor(Y[[row]]) \n",
    "        # Forward pass.\n",
    "        y_pred = x @ w + b#compute predicted value for x\n",
    "        err =  (y_pred-y)**2  \n",
    "\n",
    "        err.backward()\n",
    "        opt.step()\n",
    "        # For statistics.\n",
    "        sum_err += err.item()\n",
    "\n",
    "    mse = sum_err / X.shape[0]\n",
    "    print(f'Epoch {i+1}: MSE =', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3f221d",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56be71d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    \n",
    "    # Constructor. Just store the input values.\n",
    "    def __init__(self, data, requires_grad=False, grad_fn=None):\n",
    "        self.data = data\n",
    "        self.shape = data.shape\n",
    "        self.grad_fn = grad_fn\n",
    "        self.requires_grad = requires_grad\n",
    "        self.grad = None\n",
    "        \n",
    "    # So that we can print the object or show it in a notebook cell.\n",
    "    def __repr__(self):\n",
    "        dstr = repr(self.data)\n",
    "        if self.requires_grad:\n",
    "            gstr = ', requires_grad=True'\n",
    "        elif self.grad_fn is not None:\n",
    "            gstr = f', grad_fn={self.grad_fn}'\n",
    "        else:\n",
    "            gstr = ''\n",
    "        return f'Tensor({dstr}{gstr})'\n",
    "    \n",
    "    # Extract one numerical value from this tensor.\n",
    "    def item(self):\n",
    "        return self.data.item()    \n",
    "    \n",
    "    \n",
    "    # For Task 2:\n",
    "    \n",
    "    # Operator +\n",
    "    def __add__(self, right):\n",
    "        # Call the helper function defined below.\n",
    "        return addition(self, right)\n",
    "\n",
    "    # Operator -\n",
    "    def __sub__(self, right):\n",
    "        return subtraction(self, right)\n",
    "                \n",
    "    # Operator @\n",
    "    def __matmul__(self, right):\n",
    "        return matmul(self, right)\n",
    "\n",
    "    # Operator *\n",
    "    def __mul__(self, right):\n",
    "        return mul(self, right)\n",
    "    def __rmul__(self, right):\n",
    "        return mul(self, right)        \n",
    "    # Operator **\n",
    "    def __pow__(self, right):\n",
    "        # NOTE! We are assuming that right is an integer here, not a Tensor!\n",
    "        if not isinstance(right, int):\n",
    "            raise Exception('only integers allowed')\n",
    "        if right < 2:\n",
    "            raise Exception('power must be >= 2')\n",
    "        return power(self, right)\n",
    "\n",
    "    \n",
    "    # Backward computations. Will be implemented in Task 4.\n",
    "    def backward(self, grad_output=None):\n",
    "        if self.grad_fn is not None:\n",
    "\n",
    "            if grad_output is None:\n",
    "                self.grad_fn.backward(grad_output=1)\n",
    "            else:\n",
    "                self.grad_fn.backward(grad_output)\n",
    "        else:\n",
    "            if self.requires_grad:\n",
    "                if type(grad_output) == int:\n",
    "                    self.grad = np.array(grad_output)\n",
    "                else:\n",
    "                    self.grad = grad_output\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        \n",
    "# A small utility where we simply create a Tensor object. We use this to \n",
    "# mimic torch.tensor.\n",
    "def tensor(data, requires_grad=False):\n",
    "    return Tensor(data, requires_grad)\n",
    "        \n",
    "# We define helper functions to implement the various arithmetic operations.\n",
    "\n",
    "# This function takes two tensors as input, and returns a new tensor holding\n",
    "# the result of an element-wise addition on the two input tensors.\n",
    "def addition(left, right):\n",
    "    new_data = left.data + right.data \n",
    "    grad_fn = AddNode(left, right)\n",
    "    return Tensor(new_data, grad_fn=grad_fn)\n",
    "\n",
    "def subtraction(left, right):\n",
    "    new_data = left.data - right.data\n",
    "    grad_fn = SubNode(left, right)\n",
    "    return Tensor(new_data, grad_fn=grad_fn)\n",
    "\n",
    "def matmul(left, right):\n",
    "    if left.shape == [1,1] or right.shape == [1,1] or left.shape == () or right.shape == ():\n",
    "        new_data = left.data @ right.data\n",
    "    elif left.shape[-1] == right.shape[0]:\n",
    "        new_data = left.data @ right.data\n",
    "    else:\n",
    "        raise Exception(f\"Shape of matrises do not match: {left.shape} does not match {right.shape}\")\n",
    "    grad_fn = MatMulNode(left, right)\n",
    "    return Tensor(new_data, grad_fn=grad_fn)\n",
    "\n",
    "def mul(left, right):\n",
    "    if isinstance(left,(int,float)):\n",
    "        new_data = left * right.data\n",
    "        grad_fn = right.grad_fn\n",
    "        \n",
    "    else:\n",
    "        new_data = left.data * right  \n",
    "        grad_fn = left.grad_fn\n",
    "        \n",
    "    return Tensor(new_data, grad_fn=grad_fn)    \n",
    "\n",
    "def power(left, right):\n",
    "    new_data = left.data ** right\n",
    "    grad_fn = PowNode(left, right)\n",
    "    return Tensor(new_data, grad_fn=grad_fn)\n",
    "\n",
    "def tanh(x):\n",
    "    new_data = np.tanh(x.data)\n",
    "    grad_fn = TanhNode(x)\n",
    "    return Tensor(new_data, grad_fn=grad_fn)\n",
    "\n",
    "def sigmoid(x):\n",
    "    new_data = 1/(1+np.exp((-1)*x.data))\n",
    "    return new_data\n",
    "\n",
    "def cross_entropy(x, y):\n",
    "    new_data = (-1)*(y.data*np.log(sigmoid(x))-(1-y.data)*np.log(1-sigmoid(x)))\n",
    "    grad_fn = CrossEntropyNode(x, y)\n",
    "    return Tensor(new_data, grad_fn=grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d0f04c",
   "metadata": {},
   "source": [
    "Some sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2014827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test of addition: [[2. 3.]] + [[1. 4.]] = [[3. 7.]]\n",
      "Test of subtraction: [[2. 3.]] - [[1. 4.]] = [[ 1. -1.]]\n",
      "Test of power: [[1. 4.]] ** 2 = [[ 1. 16.]]\n",
      "Test of matrix multiplication: [[2. 3.]] @ [[-1. ]\n",
      " [ 1.2]] = [[1.6]]\n"
     ]
    }
   ],
   "source": [
    "# Two tensors holding row vectors.\n",
    "x1 = tensor(np.array([[2.0, 3.0]]))\n",
    "x2 = tensor(np.array([[1.0, 4.0]]))\n",
    "# A tensors holding a column vector.\n",
    "w = tensor(np.array([[-1.0], [1.2]]))\n",
    "\n",
    "# Test the arithmetic operations.\n",
    "test_plus = x1 + x2\n",
    "test_minus = x1 - x2\n",
    "test_power = x2 ** 2\n",
    "test_matmul = x1 @ w\n",
    "\n",
    "print(f'Test of addition: {x1.data} + {x2.data} = {test_plus.data}')\n",
    "print(f'Test of subtraction: {x1.data} - {x2.data} = {test_minus.data}')\n",
    "print(f'Test of power: {x2.data} ** 2 = {test_power.data}')\n",
    "print(f'Test of matrix multiplication: {x1.data} @ {w.data} = {test_matmul.data}')\n",
    "\n",
    "# Check that the results are as expected. Will crash if there is a miscalculation.\n",
    "assert(np.allclose(test_plus.data, np.array([[3.0, 7.0]])))\n",
    "assert(np.allclose(test_minus.data, np.array([[1.0, -1.0]])))\n",
    "assert(np.allclose(test_power.data, np.array([[1.0, 16.0]])))\n",
    "assert(np.allclose(test_matmul.data, np.array([[1.6]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c645c32",
   "metadata": {},
   "source": [
    "# Tasks 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9133db2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output\n",
    "        \n",
    "    def __repr__(self):        \n",
    "        return str(type(self))  \n",
    "\n",
    "class AddNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        \n",
    "    def backward(self, grad_output):\n",
    "        self.left.backward(grad_output)\n",
    "        self.right.backward(grad_output)\n",
    "\n",
    "class SubNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def backward(self, grad_output):        \n",
    "        self.left.backward(grad_output)\n",
    "        self.right.backward(grad_output)\n",
    "\n",
    "\n",
    "class MatMulNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.left.backward(grad_output @ self.right.data.T)\n",
    "        self.right.backward(self.left.data.T @ grad_output)\n",
    "\n",
    "class PowNode(Node):\n",
    "    def __init__(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.left.backward(grad_output*self.right*self.left.data)\n",
    "\n",
    "class TanhNode(Node):\n",
    "    def __init__(self, inp):\n",
    "        self.inp = inp\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.inp.backward(grad_output*(1-np.tanh(self.inp.data)**2))\n",
    "\n",
    "class CrossEntropyNode(Node):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        self.x.backward(grad_output*(-1*(self.y.data*sigmoid(-1*self.x) - (1 - self.y.data)*sigmoid(self.x))))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc1bb77-e869-4e08-8996-3674eed101e6",
   "metadata": {},
   "source": [
    "Sanity check for Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3276aba-4def-421b-b12e-bf0d7120f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computational graph top node after x + w1 + w2: <class '__main__.AddNode'>\n"
     ]
    }
   ],
   "source": [
    "x = tensor(np.array([[2.0, 3.0]]))\n",
    "w1 = tensor(np.array([[1.0, 4.0]]), requires_grad=True)\n",
    "w2 = tensor(np.array([[3.0, -1.0]]), requires_grad=True)\n",
    "\n",
    "test_graph = x + w1 + w2\n",
    "\n",
    "print('Computational graph top node after x + w1 + w2:', test_graph.grad_fn)\n",
    "\n",
    "assert(isinstance(test_graph.grad_fn, AddNode))\n",
    "assert(test_graph.grad_fn.right is w2)\n",
    "assert(test_graph.grad_fn.left.grad_fn.left is x)\n",
    "assert(test_graph.grad_fn.left.grad_fn.right is w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529a9bfb-ea55-4bce-9356-4956316e1904",
   "metadata": {},
   "source": [
    "Sanity check for Task 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5779ebc3-345a-48bd-a0a0-25738b101b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of loss w.r.t. w =\n",
      " [[5.6]\n",
      " [8.4]]\n"
     ]
    }
   ],
   "source": [
    "x = tensor(np.array([[2.0, 3.0]]))\n",
    "w = tensor(np.array([[-1.0], [1.2]]), requires_grad=True)\n",
    "y = tensor(np.array([[0.2]]))\n",
    "\n",
    "# We could as well write simply loss = (x @ w - y)**2\n",
    "# We break it down into steps here if you need to debug.\n",
    "\n",
    "model_out = x @ w \n",
    "diff = model_out - y\n",
    "loss = diff ** 2\n",
    "test = tanh(loss)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('Gradient of loss w.r.t. w =\\n', w.grad)\n",
    "\n",
    "assert(np.allclose(w.grad, np.array([[5.6], [8.4]])))\n",
    "assert(x.grad is None)\n",
    "assert(y.grad is None)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a8f94f9-1c34-41b0-a2b6-b6a5d03c424d",
   "metadata": {},
   "source": [
    "An equivalent cell using PyTorch code. Your implementation should give the same result for `w.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cabcc94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.6000],\n",
       "        [8.4000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_x = torch.tensor(np.array([[2.0, 3.0]]))\n",
    "pt_w = torch.tensor(np.array([[-1.0], [1.2]]), requires_grad=True)\n",
    "pt_y = torch.tensor(np.array([[0.2]]))\n",
    "\n",
    "pt_model_out = pt_x @ pt_w \n",
    "pt_model_out.retain_grad() # Keep the gradient of intermediate nodes for debugging.\n",
    "\n",
    "pt_diff = pt_model_out - pt_y\n",
    "pt_diff.retain_grad()\n",
    "\n",
    "pt_loss = pt_diff ** 2\n",
    "pt_loss.retain_grad()\n",
    "\n",
    "pt_loss.backward()\n",
    "pt_w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5439b",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b03a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "        \n",
    "    def step(self):\n",
    "        for p in self.params:\n",
    "            p.data = p.data - p.grad\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, lr, stepLimit = 100):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "        self.stepLimit = stepLimit\n",
    "        \n",
    "    def step(self):\n",
    "        for p in self.params:\n",
    "            p.data = p.data - self.lr*p.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c91898f4-48f3-4a97-82d5-314d075d6012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: MSE = 0.7999661130823178\n",
      "Epoch 2: MSE = 0.017392390107906875\n",
      "Epoch 3: MSE = 0.009377418010839892\n",
      "Epoch 4: MSE = 0.009355326971438456\n",
      "Epoch 5: MSE = 0.009365440968904256\n",
      "Epoch 6: MSE = 0.009366989180952533\n",
      "Epoch 7: MSE = 0.009367207398577986\n",
      "Epoch 8: MSE = 0.009367238983974489\n",
      "Epoch 9: MSE = 0.009367243704122532\n",
      "Epoch 10: MSE = 0.009367244427185763\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "w_init = np.random.normal(size=(2, 1)) \n",
    "b_init = np.random.normal(size=(1, 1))\n",
    "\n",
    "# We just declare the parameter tensors. Do not use nn.Linear.\n",
    "w = tensor(w_init, requires_grad = True)# a tensor initialized as w_init This is the weight\n",
    "b = tensor(b_init, requires_grad = True)# a tensor initialized as b_init This is the bias\n",
    "\n",
    "eta = 1e-2\n",
    "opt = SGD(params=[w,b] ,lr=eta)# a SGD optimizer with a learning rate of eta\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    sum_err = 0\n",
    "\n",
    "    for row in range(X.shape[0]):\n",
    "        opt.zero_grad()\n",
    "        x = tensor(X[[row], :]) \n",
    "                                \n",
    "        y = tensor(Y[[row]]) \n",
    "        # Forward pass.\n",
    "        y_pred = x @ w + b#compute predicted value for x\n",
    "        err =  (y_pred-y)**2  \n",
    "\n",
    "        err.backward()\n",
    "        opt.step()\n",
    "        # For statistics.\n",
    "        sum_err += err.item()\n",
    "\n",
    "    mse = sum_err / X.shape[0]\n",
    "    print(f'Epoch {i+1}: MSE =', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bef171",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da62980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "a4data = pd.read_csv('data/raisins.csv')\n",
    "\n",
    "X = scale(a4data.drop(columns='Class'))\n",
    "Y = 1.0*(a4data.Class == 'Besni').to_numpy()\n",
    "\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(len(Y))\n",
    "X = X[shuffle]\n",
    "Y = Y[shuffle]\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, random_state=0, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49a1b37d-3801-41b4-86ae-95e880e40f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8555555555555555\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "w1_init = np.random.normal(size=(7, 3)) \n",
    "b1_init = np.random.normal(size=(1, 1))\n",
    "w2_init = np.random.normal(size=(3, 1))\n",
    "b2_init = np.random.normal(size=(1, 1))\n",
    "\n",
    "w1 = tensor(w1_init, requires_grad = True)\n",
    "b1 = tensor(b1_init, requires_grad = True)\n",
    "\n",
    "w2 = tensor(w2_init, requires_grad = True)\n",
    "b2 = tensor(b2_init, requires_grad = True)\n",
    "\n",
    "opt = Optimizer(params = [w1,w2,b1,b2])\n",
    "# l0         l1         l2      l3        l4                         \n",
    "\n",
    "# x1---\\\n",
    "# x2----\\              h()\n",
    "# x3-----\\           /    \\\n",
    "# x4 -----> w1 -----> h() -----> w2 -----> Y\n",
    "# x5-----/           \\    /\n",
    "# x6---/              h()\n",
    "# x7-/\n",
    "\n",
    "\n",
    "#train 1 hidden layer node\n",
    "for i in range(10):\n",
    "    for row in range(Xtrain.shape[0]):\n",
    "        opt.zero_grad()\n",
    "        x = tensor(Xtrain[[row], :])\n",
    "        y = tensor(Ytrain[row])\n",
    "        \n",
    "        l1 = tanh(x@w1 + b1)\n",
    "        output = l1@w2 +b2\n",
    "\n",
    "        loss = cross_entropy(output, y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "correct_guesses = 0\n",
    "wrong_guesses = 0\n",
    "\n",
    "for row in range(Xtest.shape[0]):\n",
    "    x = tensor(Xtest[[row], :])\n",
    "    y = tensor(Ytest[row])\n",
    "        \n",
    "    l1 = tanh(x@w1 + b1)\n",
    "    output = l1@w2 +b\n",
    "\n",
    "    if (sigmoid(output) < 0.5) & (y.data == 0):\n",
    "        correct_guesses += 1\n",
    "    elif (sigmoid(output) > 0.5) & (y.data == 1):\n",
    "        correct_guesses += 1\n",
    "    else:\n",
    "        wrong_guesses +=1\n",
    "\n",
    "accuracy = correct_guesses/Xtest.shape[0]\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
